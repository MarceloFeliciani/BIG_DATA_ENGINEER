{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCÍCIO - RDD com Partições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-spark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f952cc969b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-spark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entrada1.txt\r\n",
      "entrada2.txt\r\n",
      "spark--org.apache.spark.deploy.master.Master-1-jupyter-notebook.out\r\n"
     ]
    }
   ],
   "source": [
    "# listando os arquivos que estão dentro do container jupyter-spark\n",
    "!ls /opt/spark/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Command: /opt/java/bin/java -cp /etc/spark/:/opt/spark/jars/*:/etc/hadoop/:/etc/hadoop/*:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/hadoop/hdfs/*:/opt/hadoop/share/hadoop/hdfs/lib/*:/opt/hadoop/share/hadoop/yarn/lib/*:/opt/hadoop/share/hadoop/yarn/*:/opt/hadoop/share/hadoop/mapreduce/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/tools/lib/* -Xmx1g org.apache.spark.deploy.master.Master --host localhost --port 7077 --webui-port 8080\r\n",
      "========================================\r\n",
      "20/03/18 20:31:09 INFO master.Master: Started daemon with process name: 1087@jupyter-notebook\r\n",
      "20/03/18 20:31:09 INFO util.SignalUtils: Registered signal handler for TERM\r\n",
      "20/03/18 20:31:09 INFO util.SignalUtils: Registered signal handler for HUP\r\n",
      "20/03/18 20:31:09 INFO util.SignalUtils: Registered signal handler for INT\r\n",
      "20/03/18 20:31:10 INFO spark.SecurityManager: Changing view acls to: root\r\n",
      "20/03/18 20:31:10 INFO spark.SecurityManager: Changing modify acls to: root\r\n",
      "20/03/18 20:31:10 INFO spark.SecurityManager: Changing view acls groups to: \r\n",
      "20/03/18 20:31:10 INFO spark.SecurityManager: Changing modify acls groups to: \r\n",
      "20/03/18 20:31:10 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\r\n",
      "20/03/18 20:31:10 INFO util.Utils: Successfully started service 'sparkMaster' on port 7077.\r\n",
      "20/03/18 20:31:10 INFO master.Master: Starting Spark master at spark://localhost:7077\r\n",
      "20/03/18 20:31:10 INFO master.Master: Running Spark version 2.4.1\r\n",
      "20/03/18 20:31:10 INFO util.log: Logging initialized @1454ms\r\n",
      "20/03/18 20:31:10 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\r\n",
      "20/03/18 20:31:10 INFO server.Server: Started @1504ms\r\n",
      "20/03/18 20:31:10 INFO server.AbstractConnector: Started ServerConnector@5526ec85{HTTP/1.1,[http/1.1]}{0.0.0.0:8080}\r\n",
      "20/03/18 20:31:10 INFO util.Utils: Successfully started service 'MasterUI' on port 8080.\r\n",
      "20/03/18 20:31:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@78c62b96{/app,null,AVAILABLE,@Spark}\r\n",
      "20/03/18 20:31:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1330e23c{/app/json,null,AVAILABLE,@Spark}\r\n",
      "20/03/18 20:31:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f7fb168{/,null,AVAILABLE,@Spark}\r\n",
      "20/03/18 20:31:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@50e5c33c{/json,null,AVAILABLE,@Spark}\r\n",
      "20/03/18 20:31:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44d0ea1e{/static,null,AVAILABLE,@Spark}\r\n",
      "20/03/18 20:31:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a362185{/app/kill,null,AVAILABLE,@Spark}\r\n",
      "20/03/18 20:31:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@222db61{/driver/kill,null,AVAILABLE,@Spark}\r\n",
      "20/03/18 20:31:10 INFO ui.MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://jupyter-notebook:8080\r\n",
      "20/03/18 20:31:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58cb287e{/metrics/master/json,null,AVAILABLE,@Spark}\r\n",
      "20/03/18 20:31:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3d2d7a89{/metrics/applications/json,null,AVAILABLE,@Spark}\r\n",
      "20/03/18 20:31:10 INFO master.Master: I have been elected leader! New state: ALIVE\r\n"
     ]
    }
   ],
   "source": [
    "# listando o conteúdo do arquivo abaixo\n",
    "\n",
    "!cat /opt/spark/logs/spark--org.apache.spark.deploy.master.Master-1-jupyter-notebook.out\n",
    "\n",
    "# arquivo de texto com logs do spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ler com RDD os arquivos localmente do diretório “/opt/spark/logs/” (\"file:///opt/spark/logs/\") com 10 partições\n",
    "\n",
    "### Todas as ações terão 10 partições, por o RDD foi criado com 10 partições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criado o RDD\n",
    "logs_particoes = sc.textFile(\"file:///opt/spark/logs/spark--org.apache.spark.deploy.master.Master-1-jupyter-notebook.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# por padrão cria com 2 partições\n",
    "logs_particoes.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criado o RDD com 10 Partições\n",
    "\n",
    "logs_particoes = sc.textFile(\"file:///opt/spark/logs/spark--org.apache.spark.deploy.master.Master-1-jupyter-notebook.out\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criado com 10 partições\n",
    "logs_particoes.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando as palavras\n",
    "palavras_particoes = logs_particoes.flatMap(lambda linha: linha.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# criou com 10 partições\n",
    "\n",
    "palavras_particoes.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark',\n",
       " 'Command:',\n",
       " '/opt/java/bin/java',\n",
       " '-cp',\n",
       " '/etc/spark/:/opt/spark/jars/*:/etc/hadoop/:/etc/hadoop/*:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/hadoop/hdfs/*:/opt/hadoop/share/hadoop/hdfs/lib/*:/opt/hadoop/share/hadoop/yarn/lib/*:/opt/hadoop/share/hadoop/yarn/*:/opt/hadoop/share/hadoop/mapreduce/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/tools/lib/*',\n",
       " '-Xmx1g']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separou as palavras em linhas dentro de uma lista\n",
    "\n",
    "# visualizando as 6 primeiras posições de memória\n",
    "\n",
    "palavras_particoes.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depois de modificado com o flatMap, utilizar o Map para modificar as palavras\n",
    "\n",
    "palavras_minusculas_particoes = palavras_particoes.map(lambda palavra: palavra.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# criou com 10 partições\n",
    "palavras_minusculas_particoes.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark',\n",
       " 'command:',\n",
       " '/opt/java/bin/java',\n",
       " '-cp',\n",
       " '/etc/spark/:/opt/spark/jars/*:/etc/hadoop/:/etc/hadoop/*:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/hadoop/hdfs/*:/opt/hadoop/share/hadoop/hdfs/lib/*:/opt/hadoop/share/hadoop/yarn/lib/*:/opt/hadoop/share/hadoop/yarn/*:/opt/hadoop/share/hadoop/mapreduce/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/tools/lib/*']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as palavras nas linhas estão em minúsculo\n",
    "\n",
    "palavras_minusculas_particoes.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contando a quantidade de palavras em ordem descrescente\n",
    "palavras_count_particoes = palavras_minusculas_particoes.map(lambda palavra: (palavra,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palavras_count_particoes.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark', 1),\n",
       " ('command:', 1),\n",
       " ('/opt/java/bin/java', 1),\n",
       " ('-cp', 1),\n",
       " ('/etc/spark/:/opt/spark/jars/*:/etc/hadoop/:/etc/hadoop/*:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/hadoop/hdfs/*:/opt/hadoop/share/hadoop/hdfs/lib/*:/opt/hadoop/share/hadoop/yarn/lib/*:/opt/hadoop/share/hadoop/yarn/*:/opt/hadoop/share/hadoop/mapreduce/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/tools/lib/*',\n",
       "  1),\n",
       " ('-xmx1g', 1),\n",
       " ('org.apache.spark.deploy.master.master', 1),\n",
       " ('--host', 1),\n",
       " ('localhost', 1),\n",
       " ('--port', 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palavras em minúscilas e contando a quantidade\n",
    "palavras_count_particoes.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando o ReduceByKey pode ser atribuído um número específico de partições\n",
    "### 2. Contar a quantidade de cada palavras em ordem decrescente do RDD em 5 partições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_reduce_particoes = palavras_count_particoes.reduceByKey(lambda chave1, chave2: chave1 + chave2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Continua tendo 10 partições\n",
    "palavras_reduce_particoes.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIANDO COM 5 PARTIÇÕES\n",
    "palavras_reduce_particoes = palavras_count_particoes.reduceByKey(lambda chave1, chave2: chave1 + chave2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CRIADO tendo 5 partições\n",
    "palavras_reduce_particoes.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('--port', 1),\n",
       " ('20:31:09', 4),\n",
       " ('daemon', 1),\n",
       " ('process', 1),\n",
       " ('registered', 3),\n",
       " ('signal', 3),\n",
       " ('20:31:10', 24),\n",
       " ('to:', 4),\n",
       " ('groups', 4),\n",
       " ('', 4)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palavras_reduce_particoes.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palavras_reduce_particoes.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Salvar o RDD no diretório do HDFS /user/<seu-nome>/logs_count_word_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_reduce_particoes.saveAsTextFile(\"/user/feliciani/logs_count_word_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\r\n",
      "-rw-r--r--   2 root supergroup          0 2021-06-28 18:35 /user/feliciani/logs_count_word_5/_SUCCESS\r\n",
      "-rw-r--r--   2 root supergroup        575 2021-06-28 18:35 /user/feliciani/logs_count_word_5/part-00000\r\n",
      "-rw-r--r--   2 root supergroup        208 2021-06-28 18:35 /user/feliciani/logs_count_word_5/part-00001\r\n",
      "-rw-r--r--   2 root supergroup        444 2021-06-28 18:35 /user/feliciani/logs_count_word_5/part-00002\r\n",
      "-rw-r--r--   2 root supergroup        772 2021-06-28 18:35 /user/feliciani/logs_count_word_5/part-00003\r\n",
      "-rw-r--r--   2 root supergroup        785 2021-06-28 18:35 /user/feliciani/logs_count_word_5/part-00004\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/feliciani/logs_count_word_5\n",
    "\n",
    "# CRIADO COM AS 5 PARTIÇÕES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Refazer a questão 2, com todas as funções na mesma linha de um RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_particoes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todas as funções na mesma linha\n",
    "\n",
    "palavras_particoes_linha = logs_particoes.flatMap(lambda linha: linha.split(\" \")).map(lambda palavra: palavra.lower()).map(lambda palavra: (palavra,1)).reduceByKey(lambda chave1, chave2: chave1 + chave2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palavras_particoes_linha.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
