{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.0 - Instalação no ambiente do docker desktop\n",
    "Instalar o netcat, para utilizar uma porta no jupyter-spark para poder enviar dados via tcp.\n",
    "\n",
    "docker exec -it jupyter-spark bash\n",
    "root@jupyter-spark:/# apt update\n",
    "\n",
    "root@jupyter-spark:/# apt install netcat\n",
    "\n",
    "root@jupyter-spark:/# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0 - Criar uma aplicação para ler os dados da porta 9999 e exibir no console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando o Streaming Context\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "# from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o local e o nome da aplicação\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"Dstream Python\")\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "# criando o Streaming Context no spark de 5 em 5 segundos\n",
    "ssc = StreamingContext(sc,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criado novo Spark Context\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o Dstream a partir de uma porta de entrada tcp\n",
    "# localhost, neste caso é o container jupyter-spark\n",
    "# porta 9999 pode ser utilizado por que o netcat foi instaldo com sucesso\n",
    "\n",
    "dstream = ssc.socketTextStream(\"localhost\",9999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vai mostrar cada elemento gerado pelo dtream em RDD\n",
    "\n",
    "dstream.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### no terminal container do jupyter-spark digitar\n",
    "#### root@jupyter-spark:/# nc -lp 9999\n",
    "(vai associar os dados que estão chegando a porta 9999,  l = listem, p = porta)\n",
    "\n",
    "Todo dado inserido nesse terminal, será printado no jupyter notebook abaixo do comando ss.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start desse contexto de Dstream\n",
    "\n",
    "ssc.start() # a cada 5 segundo ele vai printar os dados que estão chegando\n",
    "\n",
    "# sleep(20 ) # o start terá feito 4 prints e passará para o stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DStream não termina. Recebe o dado e processa!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se precisar parar\n",
    "\n",
    "ssc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se precisar processar e parar:\n",
    "\n",
    "#### from time import sleep\n",
    "#### start()   - iniciará e printará a cada 5 segundos\n",
    "#### sleep(20) - após 20 segundos, terá feito 4 prints e passará para o stop()\n",
    "#### stop() - vai parar o Dstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit6a434ccf4cbd411aa9abb0f2283574a5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
