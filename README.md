## BIG DATA ENGINEER
Estudos para Engenharia de Dados no Treinamento Big Data Engineer da SEMANTIX

Informações Sobre o treinamento:
Duração do Treinamento: 19/04/2021 à 02/07/2021;
Público-alvo: Programador, analista de dados ou cientista/engenheiro da computação que esteja Interessado em ingressar no mercado na área de Engenharia para Big Data;

Conteúdo programático:
O Treinamento tem duração de 10 semanas e formado por 6 módulos, com a seguinte divisão:

### Big Data Foundations (Semana 1 e 2):
- Conhecimento de ferramentas atuais no mercado de Big Data;
- Criação e funcionamento de um cluster Hadoop para Big Data em Docker;
- Manipulação de dados com HDFS; 
- Manipulação de dados com uso do Hive;
- Otimização de consultas em grandes volumes de dados estruturados e semiestruturados com uso de Hive;
- Ingestão de dados relacionais para o HDFS/Hive, com uso do Sqoop;
- Otimização de importação no Sqoop;
- Exportação de dados do HDFS para o SGBD, com uso do Sqoop;
- Manipulação de dados com HBase;
- Operações com Dataframe em Spark para processamento de dados em batch;
- Uso do Spark SQL Queries para consultas de dados estruturados e semiestruturados.

### MongoDB - Básico (Semana 3):
- Entendimento de conceitos e arquitetura NoSQL e MongoDB;
- Instalação de cluster MongoDB através de container e Cloud;
- Manipular coleções, documentos e índices;
- Realizar diversas pesquisas no MongoDB com diferentes operadores;
- Fazer uso das interfaces gráficas MongoExpress e MongoCompass;
- Trabalhar com pipeline de agregações;
- Entendimento de Replicação e shards.

### Redis – Básico (Semana 4):
- Entendimento de conceitos e arquitetura NoSQL e Redis;
- Instalação de cluster Redis através de container;
- Manipulação de diversos tipos de estrutura de dados com Redis-CLI;
- Implementar paradigma de mensagens Pub/Sub;
- Configurações básicas de persistência de dados.

### Apache Kafka – Básico (Semana 5):
- Entendimento de conceitos e arquitetura do Kafka e da Confluent;
- Instalação de cluster Kafka através de container;
- Gerenciamento de tópicos;
- Produção e consumo de dados através do console;
- Entendimento das guias do Control Center;
- Desenvolvimento de stream com uso do KSQL;
- Aplicação de KSQL Datagen;
- Produção e consumo de dados com uso do Schema Registry;
- Trabalhando com Kafka Connect;
- Custos com Confluent Cloud;
- Otimização de parâmetros;
- Melhores práticas em um cluster Kafka.

### Elastic Essential I (Semana 6 e 7):
- Entendimento de conceitos e arquitetura da Elastic;
- Instalação de cluster Elastic através de container;
- Realizar operações de CRUD em índices;
- Gerenciamento de índices;
- Alteração de mapeamento e reindex;
- Desenvolvimento de consultas do tipo term, terms, range, match e multi_match, com uso de bool query;
- Aplicação de analyzers em atributos;
- Desenvolvimento de agregações básicas;
- Ingestão de dados através de beats e logstash;
- Entendimento das guias do Kibana;

### Spark - Big Data Processing (Semana 8, 9 e 10)
- Uso do Jupyter Notebooks para a criação de projetos em Spark com Python
- Spark batch intermediario
- Operações com RDD em Spark para processamento de dados em batch;
- Uso de Partições com RDD;
- Operações com Dataset em Spark para processamento de dados em batch;
- Uso de Dataset em Dataframe e RDD;
- Comandos avançados com Dataset;
- Uso do IntelliJ IDEA para a criação de projetos em Spark com Scala;
- Struct Streaming para leitura de dados do Kafka;
- Spark Streaming para leitura de dados do Kafka;
- Otimizações com uso de Variáveis Compartilhadas;
- Criações de User defined Function;
- Configurações de Tunning para o Spark Application.



### Requisitos mínimos para o treinamento:
- Conhecimento intermediário de pelo menos uma destas linguagens:
- Python (Preferível);
- Scala;
- Java.
- Conhecimento intermediário em SQL;
- Conhecimento básico em Git.
